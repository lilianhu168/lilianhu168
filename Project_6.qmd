---
title: "Twitter Finance Text Analysis +  Sentiment Extension"
author: "Lilian Hu"
subtitle: "May 2025"
format:
  revealjs:
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
---

## Project 2 Overview

-   **Context:** X (formerly Twitter) as a real-time platform for financial discussions
-   **Datasets:**
    1.  **Financial Tweets** from Kaggle (tweets, timestamps, metadata)\
    2.  **Company lookup table** (`stocks_cleaned.csv`) mapping tickers to company names
    3.  **Key question:** Which stocks dominate financial discussions on X, and how are they co-mentioned?

## Data Wrangling & Cleaning

-   Lowercase all tax (`str_to_lower()`)
-   Remove URLs (`str_replace_all("https?://\\S+", "")`
-   Strip punctuation (`str_replace_all("[[:punct:]]", " ")`)
-   regex `"\\$[A-Za-z]{1,6}"` → remove `$`, uppercase tickers
-   **Mapping:** join with `stocks_cleaned` for `company_name`

```{r}
library(tidyverse)
stocks_cleaned <- read.csv("stocks_cleaned.csv")
tweets <- read_csv("stockerbot_export.csv") |>
  select(text, timestamp, source) |>
  rename(timestamp_original = timestamp) |>
  mutate(
    text_clean = text |>
      str_to_lower() |>
      str_replace_all("https?://\\S+", "") |>
      str_replace_all("[[:punct:]]", " ")
  ) |>
  filter(!is.na(text_clean))

stocks_cleaned <- stocks_cleaned |> 
  rename(ticker = ticker, company_name = name) |>
  mutate(ticker = str_to_upper(ticker))

tweets <- tweets |>
  mutate(tickers_found = str_extract_all(text_clean, "\\$[A-Za-z]{1,6}")) |>
  unnest(tickers_found) |>
  mutate(tickers_found = str_remove(tickers_found, "\\$") |> str_to_upper()) |>
  left_join(stocks_cleaned, by = c("tickers_found" = "ticker")) |>
  filter(!is.na(company_name))

ticker_counts <- tweets |>
  count(company_name, sort = TRUE)
```

## Top Mentions

```{r top-mentions, echo=FALSE, fig.width=14, fig.height=8, out.width="100%"}
top_companies <- ticker_counts |> slice_max(n, n = 30)

ggplot(top_companies, aes(x = reorder(company_name, -n), y = n)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "Top 30 Mentioned Companies in Financial Tweets",
    x = "Company Name",
    y = "Number of Mentions"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Co-Occurrences

```{r, echo=FALSE, fig.width=14, fig.height=8, out.width="100%"}
tweets_with_multiple_tickers <- tweets |>
  group_by(text_clean) |>
  summarise(tickers = list(unique(tickers_found)), .groups = "drop") |>
  filter(lengths(tickers) > 1)

stock_pairs <- tweets_with_multiple_tickers |>
  mutate(pairs = map(tickers, ~combn(.x, 2, simplify = FALSE))) |>
  unnest(pairs) |>
  transmute(ticker1 = map_chr(pairs, 1), ticker2 = map_chr(pairs, 2)) |>
  count(ticker1, ticker2, sort = TRUE)

top_stock_pairs <- stock_pairs |> slice_max(order_by = n, n = 10)

ggplot(top_stock_pairs, aes(x = ticker1, y = ticker2, size = n)) +
  geom_point() +
  labs(
    title = "Top 10 Most Commonly Co-Mentioned Stocks",
    x = "Stock 1",
    y = "Stock 2",
    size = "Number of Co-Mentions"
  ) +
  theme_minimal()
```

## Results & Key Findings

-   **Dominant mentions:** Netflix, Amazon, Alphabet (Google), Facebook, Microsoft, Apple
-   **Interpretation:** High mention volumes reflect investor focus on earnings, news, and events.
-   **Co-occurrence:** Pairs like Apple--Microsoft and Amazon--Alphabet indicate sector groupings.

## Extra: Sentiment Analysis

**Goal:** Gauge positive vs. negative tone in financial tweets.

 - **Approach:**\
1. Tokenize cleaned tweet text.\
2. Join tokens with Bing sentiment lexicon.\
3. Compute net sentiment per ticker (positive − negative counts).

## Sentiment Analysis

- Tidytext
- Using Loughran-McDonald lexicon
- inner join 

```{r}

library(tidyverse)
library(tidytext)
library(textdata)

if (!"tweet_id" %in% names(tweets)) {
  tweets <- tweets |> mutate(tweet_id = row_number())
}

tweet_words <- tweets |> 
  select(tweet_id, company_name, text_clean) |> 
  unnest_tokens(word, text_clean)

lm_lex <- lexicon_loughran() |> 
  filter(sentiment %in% c("positive", "negative"))

tweet_sent_fin <- tweet_words |> 
  inner_join(lm_lex, by = "word") |> 
  mutate(score = if_else(sentiment == "positive", 1, -1))

# ── 3  Score tweets & companies ─────────────────────────────────
tweet_scores_fin <- tweet_sent_fin |> 
  group_by(tweet_id, company_name) |> 
  summarise(tweet_score = sum(score), .groups = "drop")

company_fin <- tweet_scores_fin |> 
  group_by(company_name) |> 
  summarise(net_sent = sum(tweet_score),
            n_tweets = n(), .groups = "drop") |> 
  filter(n_tweets >= 50)          

```
___ 
```{r}

top20_fin <- company_fin |> 
  slice_max(order_by = abs(net_sent), n = 20)

ggplot(top20_fin,
       aes(x = reorder(company_name, .data$net_sent),
           y = net_sent,
           fill = net_sent > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Net sentiment in tweets (Loughran-McDonald lexicon)",
       x = "Company",
       y = "Positive – Negative word count") +
  scale_fill_manual(values = c("TRUE" = "steelblue",
                               "FALSE" = "firebrick")) +
  theme_minimal()


```

